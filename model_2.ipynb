{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceec4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import lru_cache\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c08ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = os.path.expanduser('~/Downloads/transmissions/size_10/lead_size_10_conc_7_config_659.csv')\n",
    "data = np.loadtxt(p1, delimiter=',', skiprows=1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc59edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de03500",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1024)\n",
    "def configs(conc):\n",
    "    df = {}\n",
    "    for x in range(5000):\n",
    "        path = os.path.expanduser(f'~/Downloads/transmissions/size_10/lead_size_10_conc_{conc}_config_{x}.csv')\n",
    "        new_data = np.loadtxt(path, delimiter=',',skiprows=1)[:,1]\n",
    "        df[x] = new_data\n",
    "    df = pd.DataFrame(df)\n",
    "    df.index = np.arange(0, 4, 0.01)\n",
    "    return df.clip(upper=size)\n",
    "\n",
    "\n",
    "DEVICE_COMBS = {}\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def chosen_for_config(n, size, config):\n",
    "    width = int(size); n = int(n); cfg = int(config)\n",
    "    if width not in DEVICE_COMBS:\n",
    "        DEVICE_COMBS[width] = np.array([(i, j) for i in range(100) for j in range(width)], dtype=int)\n",
    "    device_combs = DEVICE_COMBS[width]\n",
    "    rng = np.random.RandomState(cfg)\n",
    "    chosen_indices = rng.choice(len(device_combs), size=n, replace=False)\n",
    "    return device_combs[chosen_indices]\n",
    "\n",
    "\n",
    "def possible_combs(n, width):\n",
    "    def combs_for_seed(x):\n",
    "        return chosen_for_config(n, width, x)\n",
    "    return combs_for_seed\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def distance_matrix(conc,config):\n",
    "    imps  = possible_combs(conc, size)\n",
    "    item = imps(config)\n",
    "    #print(np.diag(item[:,1]))\n",
    "    x = np.stack((size - item[:,1],item[:,1]), axis=1)\n",
    "    return squareform(pdist(item, metric='euclidean')) + np.diag(x.min(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fde084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.        , 27.65863337,  8.06225775, 30.        , 30.14962686,\n",
       "        33.73425559, 33.13608305],\n",
       "       [27.65863337,  1.        , 35.35533906, 57.31491952, 57.07889277,\n",
       "        60.00833275,  6.70820393],\n",
       "       [ 8.06225775, 35.35533906,  4.        , 22.02271555, 22.09072203,\n",
       "        25.70992026, 41.0487515 ],\n",
       "       [30.        , 57.31491952, 22.02271555,  3.        ,  3.        ,\n",
       "         7.61577311, 63.07138812],\n",
       "       [30.14962686, 57.07889277, 22.09072203,  3.        ,  4.        ,\n",
       "         5.        , 63.        ],\n",
       "       [33.73425559, 60.00833275, 25.70992026,  7.61577311,  5.        ,\n",
       "         0.        , 66.12110102],\n",
       "       [33.13608305,  6.70820393, 41.0487515 , 63.07138812, 63.        ,\n",
       "        66.12110102,  4.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d268c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing concentration: 7\n",
      "Processing concentration: 9\n",
      "Processing concentration: 11\n",
      "Processing concentration: 13\n",
      "Processing concentration: 15\n",
      "Processing concentration: 17\n",
      "Processing concentration: 19\n",
      "Processing concentration: 21\n",
      "Processing concentration: 23\n",
      "Processing concentration: 25\n",
      "Processing concentration: 27\n",
      "Processing concentration: 29\n",
      "Processing concentration: 31\n",
      "Processing concentration: 33\n",
      "Processing concentration: 35\n",
      "Processing concentration: 37\n",
      "Processing concentration: 39\n",
      "Processing concentration: 41\n",
      "Processing concentration: 43\n",
      "Processing concentration: 45\n",
      "Processing concentration: 47\n",
      "Processing concentration: 49\n",
      "Shape of x: (110000, 400, 2)\n",
      "Shape of y: (110000, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset():\n",
    "    x = []\n",
    "    y = []\n",
    "    for conc in np.arange(7,50,2):\n",
    "        print(f\"Processing concentration: {conc}\")\n",
    "        for config in range(5000):\n",
    "            dist_mat = 0 *np.eye(50)\n",
    "            dist_mat[:conc,:conc] +=  distance_matrix(conc, config)\n",
    "            \n",
    "            arr = configs(conc).index,configs(conc)[config]\n",
    "            arr = np.array(arr).T\n",
    "            y.append(dist_mat)\n",
    "            x.append(arr)\n",
    "\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    print(f\"Shape of x: {x.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "    return x,y\n",
    "\n",
    "x,y = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f1f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 2, 400) (110000, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "x = np.transpose(x, (0, 2, 1))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4359ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e8b831",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MatrixDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m MatrixDataset(x, y)\n\u001b[1;32m      2\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(full_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MatrixDataset' is not defined"
     ]
    }
   ],
   "source": [
    "full_dataset = MatrixDataset(x, y)\n",
    "loader = DataLoader(full_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7f57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define Custom Dataset\n",
    "class SensorDistanceDataset(Dataset):\n",
    "    \"\"\"Custom dataset for sensor data -> distance matrix prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, sensor_data, distance_matrices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sensor_data: numpy array of shape (n_samples, n_sensors, n_features)\n",
    "            distance_matrices: numpy array of shape (n_samples, n_points, n_points)\n",
    "        \"\"\"\n",
    "        self.sensor_data = torch.FloatTensor(sensor_data)\n",
    "        self.distance_matrices = torch.FloatTensor(distance_matrices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sensor_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sensor_data[idx], self.distance_matrices[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e03b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Neural Network Model\n",
    "class SensorToDistanceMatrixModel(nn.Module):\n",
    "    def __init__(self, n_sensors, n_features, matrix_size, hidden_dims=[512, 256, 128]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_sensors: number of sensors\n",
    "            n_features: number of features per sensor\n",
    "            matrix_size: size of the square distance matrix (n_points)\n",
    "            hidden_dims: list of hidden layer dimensions\n",
    "        \"\"\"\n",
    "        super(SensorToDistanceMatrixModel, self).__init__()\n",
    "        \n",
    "        self.n_sensors = n_sensors\n",
    "        self.n_features = n_features\n",
    "        self.matrix_size = matrix_size\n",
    "        self.output_size = matrix_size * matrix_size\n",
    "        \n",
    "        # Input size calculation\n",
    "        input_size = n_sensors * n_features\n",
    "        \n",
    "        # Build the network layers\n",
    "        layers = []\n",
    "        prev_dim = input_size\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, self.output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten sensor data: (batch_size, n_sensors, n_features) -> (batch_size, n_sensors * n_features)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through network\n",
    "        output = self.network(x)\n",
    "        \n",
    "        # Reshape to distance matrix: (batch_size, matrix_size, matrix_size)\n",
    "        distance_matrix = output.view(-1, self.matrix_size, self.matrix_size)\n",
    "        \n",
    "        # Ensure symmetry (distance matrices should be symmetric)\n",
    "        distance_matrix = (distance_matrix + distance_matrix.transpose(-2, -1)) / 2\n",
    "        \n",
    "        # Ensure diagonal is zero (distance from point to itself)\n",
    "        mask = torch.eye(self.matrix_size, device=distance_matrix.device).bool()\n",
    "        distance_matrix = distance_matrix.masked_fill(mask.unsqueeze(0), 0)\n",
    "        \n",
    "        return distance_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02f0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=1000, learning_rate=0.001):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (sensor_data, target_matrices) in enumerate(train_loader):\n",
    "            sensor_data, target_matrices = sensor_data.to(device), target_matrices.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sensor_data)\n",
    "            loss = criterion(outputs, target_matrices)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sensor_data, target_matrices in val_loader:\n",
    "                sensor_data, target_matrices = sensor_data.to(device), target_matrices.to(device)\n",
    "                outputs = model(sensor_data)\n",
    "                loss = criterion(outputs, target_matrices)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8622fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1831748 parameters\n",
      "Epoch [1/50], Train Loss: 247.7451, Val Loss: 893.8703\n",
      "Epoch [11/50], Train Loss: 233.6884, Val Loss: 899.9047\n",
      "Epoch [21/50], Train Loss: 229.9118, Val Loss: 902.9967\n",
      "Epoch [31/50], Train Loss: 229.0682, Val Loss: 904.6174\n",
      "Epoch [41/50], Train Loss: 229.1462, Val Loss: 904.0324\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Usage Example\n",
    "def main():\n",
    "    # Example parameters (adjust based on your data)\n",
    "    n_samples = 110000\n",
    "    n_sensors = 2\n",
    "    n_features = 400\n",
    "    matrix_size = 50  # 50x50 distance matrix\n",
    "\n",
    "    # Generate dummy data (replace with your actual data)\n",
    "    sensor_data = x\n",
    "    distance_matrices = y\n",
    "    \n",
    "    # Make distance matrices symmetric and zero diagonal\n",
    "    for i in range(n_samples):\n",
    "        distance_matrices[i] = (distance_matrices[i] + distance_matrices[i].T) / 2\n",
    "        np.fill_diagonal(distance_matrices[i], 0)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    train_sensor = sensor_data[:split_idx]\n",
    "    train_matrices = distance_matrices[:split_idx]\n",
    "    val_sensor = sensor_data[split_idx:]\n",
    "    val_matrices = distance_matrices[split_idx:]\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = SensorDistanceDataset(train_sensor, train_matrices)\n",
    "    val_dataset = SensorDistanceDataset(val_sensor, val_matrices)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Create model with your data dimensions\n",
    "    model = SensorToDistanceMatrixModel(\n",
    "        n_sensors=n_sensors,        # 2 sensors\n",
    "        n_features=n_features,      # 400 features each\n",
    "        matrix_size=matrix_size,    # 50x50 output matrix\n",
    "        hidden_dims=[1024, 512, 256, 128]  # Larger network for your data size\n",
    "    )\n",
    "    \n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "    \n",
    "    # Train model\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, num_epochs=50)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'distance_matrix_model.pth')\n",
    "    print(\"Model saved!\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Step 5: Inference Function\n",
    "def predict_distance_matrix(model, sensor_input):\n",
    "    \"\"\"Make prediction on new sensor data\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if isinstance(sensor_input, np.ndarray):\n",
    "            sensor_input = torch.FloatTensor(sensor_input)\n",
    "        \n",
    "        sensor_input = sensor_input.to(device)\n",
    "        if len(sensor_input.shape) == 2:  # Add batch dimension if needed\n",
    "            sensor_input = sensor_input.unsqueeze(0)\n",
    "        \n",
    "        predicted_matrix = model(sensor_input)\n",
    "        return predicted_matrix.cpu().numpy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, train_losses, val_losses = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85e4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
